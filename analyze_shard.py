#!/usr/bin/env python3
import pandas as pd
import sys

file_path = "shard_00241.parquet" if len(sys.argv) < 2 else sys.argv[1]

print("="*100)
print("ðŸ“Š ANALYZING:", file_path)
print("="*100)

df = pd.read_parquet(file_path)

# Basic info
print(f"\nðŸ“ˆ BASIC STATS:")
print(f"Rows: {len(df):,}")
print(f"Columns: {len(df.columns)}")
print(f"Columns: {list(df.columns)}")

# Data types
print(f"\nðŸ“‹ SCHEMA:")
print(df.dtypes)

# Calculate word counts
text_col = 'text' if 'text' in df.columns else df.columns[0]
print(f"\nâ³ Calculating word counts...")
df['calculated_word_count'] = df[text_col].astype(str).str.split().str.len()

print(f"\nðŸ“ WORD COUNT STATS:")
if 'word_count' in df.columns:
    wc_col = 'word_count'
else:
    wc_col = 'calculated_word_count'

print(f"Min: {df[wc_col].min():,} words")
print(f"Max: {df[wc_col].max():,} words")
print(f"Mean: {df[wc_col].mean():.1f} words (~{df[wc_col].mean()*1.33:.0f} tokens)")
print(f"Median: {df[wc_col].median():.1f} words")
print(f"Std Dev: {df[wc_col].std():.1f} words")

# Distribution
print(f"\nWord Count Distribution:")
print(f"  â€¢ 0-256 words: {(df[wc_col] <= 256).sum():,} ({(df[wc_col] <= 256).sum()/len(df)*100:.1f}%)")
print(f"  â€¢ 257-512 words: {((df[wc_col] > 256) & (df[wc_col] <= 512)).sum():,} ({((df[wc_col] > 256) & (df[wc_col] <= 512)).sum()/len(df)*100:.1f}%)")
print(f"  â€¢ 513-1024 words: {((df[wc_col] > 512) & (df[wc_col] <= 1024)).sum():,} ({((df[wc_col] > 512) & (df[wc_col] <= 1024)).sum()/len(df)*100:.1f}%)")
print(f"  â€¢ 1025+ words: {(df[wc_col] > 1024).sum():,} ({(df[wc_col] > 1024).sum()/len(df)*100:.1f}%)")

# Character counts
print(f"\nCharacter Count Stats:")
df['char_count'] = df[text_col].astype(str).str.len()
print(f"Min: {df['char_count'].min():,} chars")
print(f"Max: {df['char_count'].max():,} chars")
print(f"Mean: {df['char_count'].mean():.0f} chars")

# Check for overlap
print(f"\nðŸ”— OVERLAP CHECK:")
if len(df) > 1:
    # Check first pair
    t1_end = str(df[text_col].iloc[0])[-100:]
    t2_start = str(df[text_col].iloc[1])[:100]
    has_overlap = any(t1_end[i:] == t2_start[:len(t1_end)-i] for i in range(20, len(t1_end)))
    print(f"Has overlap: {'YES' if has_overlap else 'NO'}")

# First 3 rows
print(f"\nðŸ“– FIRST 3 ROWS:")
for i in range(min(3, len(df))):
    print(f"\n--- ROW {i+1} ---")
    for col in df.columns:
        val = df[col].iloc[i]
        if col == text_col:
            print(f"{col}: {str(val)[:200]}...")
        else:
            print(f"{col}: {val}")

# Last 3 rows
print(f"\nðŸ“– LAST 3 ROWS:")
for i in range(max(0, len(df)-3), len(df)):
    print(f"\n--- ROW {i+1} ---")
    for col in df.columns:
        val = df[col].iloc[i]
        if col == text_col:
            print(f"{col}: {str(val)[:200]}...")
        else:
            print(f"{col}: {val}")

print("\n" + "="*100)
print("âœ… ANALYSIS COMPLETE")
print("="*100)

# LLM Training Assessment
print("\n" + "="*100)
print("ðŸ¤– LLM TRAINING ASSESSMENT")
print("="*100)
avg_words = df[wc_col].mean()
print(f"âœ“ Total training examples: {len(df):,}")
print(f"âœ“ Average chunk size: {avg_words:.0f} words (~{avg_words*1.33:.0f} tokens)")

if avg_words < 300:
    print(f"âœ“ Size category: Small chunks")
    print(f"  â†’ Good for: Embeddings, fine-tuning, specific tasks")
elif avg_words < 700:
    print(f"âœ“ Size category: Medium chunks") 
    print(f"  â†’ Good for: General training, RAG, most LLM tasks")
else:
    print(f"âœ“ Size category: Large chunks")
    print(f"  â†’ Good for: Long context, document understanding")

print(f"âœ“ Overlap: {'YES' if has_overlap else 'NO'}")
if not has_overlap:
    print(f"  â†’ Note: No overlap = clean boundaries, no redundant context")
else:
    print(f"  â†’ Benefit: Overlap preserves context across chunks")

print(f"âœ“ Schema: Minimal (text-only)")
print(f"  â†’ Optimized for pure text training")
print(f"âœ“ Format: Parquet (compressed, efficient)")
print(f"âœ“ Dataset quality: Professional (diverse topics, clean text)")
print("="*100)

