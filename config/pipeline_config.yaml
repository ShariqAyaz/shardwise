# ShardWise Pipeline Configuration

# Paths
paths:
  raw_data: "raw_data"
  intermediate: "intermediate"
  dataset: "dataset"
  
# Text Extraction Settings
extraction:
  pdf:
    method: "pypdf"  # Options: pypdf, pdfminer
    extract_images: false
  html:
    method: "trafilatura"  # Options: trafilatura, beautifulsoup
    extract_links: false
  encoding: "utf-8"

# Cleaning and Normalisation Settings
cleaning:
  remove_urls: true
  remove_emails: true
  remove_phone_numbers: true
  normalise_whitespace: true
  normalise_quotes: true
  fix_encoding: true
  min_line_length: 10
  languages:
    - "en" 
  language_confidence_threshold: 0.8

# Content Filtering - Pattern-Based Text Redaction
# Generic regex-based filtering for data cleaning and processing
# Important: Users are responsible for ensuring compliance with applicable laws
content_filter:
  enabled: false  # Disabled by default - configure patterns before enabling
  
  # Action when patterns match
  # Options: remove (redact sections), flag (mark for review), reject (discard text)
  action: "flag"  # Default to flag for review
  
  # Pattern Configuration - Define your own regex patterns
  patterns:
    # Add any regex patterns you need for your use case
    custom_patterns: []
      # Examples:
      # - "\\b\\d{3}-\\d{2}-\\d{4}\\b"              # SSN-like patterns
      # - "\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b"  # Email addresses
      # - "(?i)page\\s+\\d+"                         # Page numbering
      # - "(?i)confidential|draft"                   # Document status
      # - "(?i)metadata_field:\\s*\\w+"             # Metadata tags
  
  # Context characters to include around matches (for boundary detection)
  context_chars: 500
  
  # Logging
  log_detections: true
  log_removals: true
  
  # Pipeline stages where filter applies
  stages:
    extraction: false
    cleaning: false
    chunking: false
    filtering: false

# Chunking Settings
chunking:
  method: "fixed"  # Options: sentence, paragraph, fixed
  
  # Chunk size limits - FLEXIBLE, set to your needs
  min_chunk_size: 50      # Minimum words (set 0 to disable, or increase to filter short chunks)
  max_chunk_size: 100000  # Maximum words (set very high to allow any size, or lower to enforce limits)
  overlap: 0              # Words to overlap between chunks (0 = no overlap, 50-200 = typical overlap)
  
  preserve_sentence_boundaries: false  # false for fixed blobs, true for sentence-based
  
  # Filtering options - control what chunks to keep
  enforce_limits: false    # true = reject chunks outside min/max, false = keep all sizes
  warn_on_outliers: true   # Log warnings for chunks outside expected range
  
  # Size recommendations based on your data analysis:
  # Your data: 769 words average (range: 43 - 82,419 words)
  # - 24.7% are 0-256 words (small)
  # - 29.1% are 257-512 words (medium-small)
  # - 28.3% are 513-1024 words (medium-large)
  # - 17.8% are 1025+ words (large)
  
  # Common presets (adjust min/max above to match):
  # Permissive (accept all):     min: 0,    max: 999999,  overlap: 0
  # Small chunks:                 min: 100,  max: 512,     overlap: 50
  # Medium chunks:                min: 256,  max: 1024,    overlap: 100
  # Large chunks:                 min: 512,  max: 2048,    overlap: 150
  # Your data profile:            min: 50,   max: 100000,  overlap: 0

# Deduplication Settings
deduplication:
  exact_dedup: true
  near_dedup: true
  minhash:
    num_perm: 128
    threshold: 0.8
  min_text_length: 50  # characters

# Quality Filtering Settings - FLEXIBLE LIMITS
quality:
  # Word count limits (based on your data: 43-82,419 words, avg 769)
  min_words: 20          # Minimum (0 = no limit, 20 = filter very short)
  max_words: 100000      # Maximum (999999 = no limit, or set lower to enforce)
  
  # Quality metrics - adjust to your tolerance
  min_unique_words_ratio: 0.0    # Vocabulary diversity (0.0 = permissive, 0.3 = strict)
  max_repetition_ratio: 1.0      # Repetition tolerance (1.0 = allow all, 0.5 = strict)
  min_alpha_ratio: 0.0           # Alphabetic character ratio (0.0 = allow all, 0.7 = strict)
  
  # Readability scoring
  calculate_readability: false   # Set true to calculate (slower), false to skip
  min_readability_score: 0       # Flesch reading ease, 0-100 (0 = no filter)
  
  # Enforcement
  strict_filtering: false        # true = reject chunks failing quality, false = warn only
  log_rejected_chunks: true      # Log what gets filtered out

# Sharding Settings - FLEXIBLE SCHEMA CONTROL
sharding:
  format: "parquet"
  max_shard_size_mb: 500      # Maximum size per shard file (adjust as needed)
  max_rows_per_shard: null    # Row limit per shard (null = size-based, number = fixed rows)
  compression: "snappy"       # Options: snappy, gzip, brotli, none
  
  # Schema configuration - YOU DECIDE what to store
  schema:
    # Predefined schemas for common use cases:
    
    # Minimal - Pure text only (like your shard_00241.parquet)
    minimal: ["text"]
    
    # Basic - Text with ID for tracking
    basic: ["id", "text"]
    
    # Standard - Text + essential metadata
    standard: ["id", "text", "source_file", "language", "word_count"]
    
    # Full - Everything available
    full: ["id", "text", "source_file", "filename", "file_type", "language", 
           "chunk_index", "word_count", "char_count", "chunking_method", 
           "chunking_timestamp", "unique_words", "unique_ratio", 
           "repetition_ratio", "alpha_ratio", "readability_score"]
    
    # ACTIVE SCHEMA - Choose what you want to store:
    active: "minimal"  # Options: minimal, basic, standard, full, custom
    
    # Custom - Define your own column list
    # Set active: "custom" and list columns below
    custom: ["text", "word_count", "language"]
    
  # Advanced options
  allow_variable_columns: true    # Allow different columns per shard
  drop_null_columns: false        # Remove columns that are all null
  include_metadata_file: true     # Save schema info in separate metadata file

# Annotation Export Settings
annotation:
  format: "jsonl"
  niches:
    - name: "general"
      keywords: []
    - name: "finance"
      keywords: ["financial", "investment", "trading", "market", "stock"]
    - name: "health"
      keywords: ["medical", "health", "disease", "treatment", "patient"]
    - name: "technology"
      keywords: ["software", "programming", "algorithm", "computer", "AI"]
    - name: "science"
      keywords: ["research", "experiment", "theory", "hypothesis", "data"]
  auto_categorise: true

# Label Studio Settings
labelstudio:
  url: "http://localhost:8080"
  api_key: ""  # Set via environment variable LABELSTUDIO_API_KEY
  project_name: "ShardWise Annotation"
  project_description: "LLM instruction fine-tuning annotation project"
  sampling_rate: 1.0  # Fraction of data to send for annotation (0.0-1.0)

# Prefect Settings
prefect:
  log_level: "INFO"
  max_retries: 3
  retry_delay_seconds: 60
  parallel_workers: 4

# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/pipeline.log"

